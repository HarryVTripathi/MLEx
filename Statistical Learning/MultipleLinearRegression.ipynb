{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm, inv\n",
    "from numpy import transpose\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "X0 = np.ones(n)\n",
    "X1 = np.random.randint(200, 800, n)\n",
    "X2 = np.random.randint(200, 800, n)\n",
    "X3 = 50 * np.random.random_sample((n,))\n",
    "X4 = (-72) * np.random.random_sample((n,))\n",
    "X5 = np.random.randint(1000, 1600, n)\n",
    "\n",
    "X_train = transpose(np.array([X0, X1, X2, X3, X4, X5]))\n",
    "Y = 16 * X0 + 2 * X1 - 7 * X2 - 6 * (X3) + 9 * X4 + X5\n",
    "Y_train = Y + np.random.randint(200, 800, len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\Harry\\\\Documents\\\\DS\\\\Datasets\\\\kc_house_data.csv')\n",
    "#df.head()\n",
    "\n",
    "X_train = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'yr_built', 'condition', 'grade', 'yr_renovated', 'sqft_living15', 'sqft_lot15']].to_numpy()\n",
    "Y_train = df['price'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression model\n",
    "\n",
    "\n",
    "#### Multiple linear regression model is described as:-\n",
    "\n",
    "\n",
    "<img src=\"1.JPG\" width=\"480\" height=\"72\"/>\n",
    "\n",
    "<p>\n",
    "    where \u000fi and ei are the random error and residual, respectively, associated with\n",
    "the response yi and fitted value ˆyi.\n",
    "</p>\n",
    "\n",
    "\n",
    "#### The squared sum of residuals is calculated as:-\n",
    "\n",
    "<img src=\"2.JPG\" width=\"450\" height=\"50\"/>\n",
    "    \n",
    "<p>\n",
    "    Differentiating SSE in turn with respect to b0, b1, . . . , bk and equating to zero, we\n",
    "generate the set of k + 1 normal equations for multiple linear regression.\n",
    "</p>\n",
    "\n",
    "<img src=\"3.JPG\" width=\"500\" height=\"175\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In general, the linear model can be written as:-\n",
    "\n",
    "<img src=\"5.JPG\" width=\"500\" height=\"140\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<p>\n",
    "    Then the least squares method for estimation of β, involves finding b for which\n",
    "    <img src=\"6.JPG\" width=\"200\" height=\"35\"/>\n",
    "    is minimized\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "#### For the partial derivative of above matrix equation, w.r.t. \"b\" to be 0\n",
    "\n",
    "<img src=\"7.JPG\" width=\"132\" height=\"44\"/>\n",
    "\n",
    "<p>\n",
    "    The result reduces to the solution of b in \n",
    "    ```\n",
    "    (X'X)b = X'y.\n",
    "    ```\n",
    "    <br>\n",
    "    <br>\n",
    "    Apart from the initial element, the ith row represents the x-values that give rise to the response yi.\n",
    "</p>\n",
    "\n",
    "<img src=\"8.JPG\" width=\"440\" height=\"176\"/>\n",
    "\n",
    "and\n",
    "\n",
    "<img src=\"9.JPG\" width=\"242\" height=\"132\"/>\n",
    "\n",
    "allows the normal equations to be put in the matrix form\n",
    "\n",
    "<img src=\"10.JPG\" width=\"80\" height=\"30\"/>\n",
    "\n",
    "\n",
    "```(X'X)b = g```\n",
    "\n",
    "<br>\n",
    "\n",
    "If the matrix A is nonsingular, we can write the solution for the regression\n",
    "coefficients as\n",
    "\n",
    "<img src=\"11.JPG\" width=\"225\" height=\"33\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Implementing regression model without using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[247.80521173   2.04983934  -7.02769393  -6.16070203   8.09241818\n",
      "   1.16343121]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2198.102558163217"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans = transpose(X_train)\n",
    "coefficient_B_array = ((inv(X_trans @ X_train)) @ X_trans) @ Y_train\n",
    "print(coefficient_B_array)\n",
    "\n",
    "X_test = np.array([1, 400, 600, 25, -36, 1200])\n",
    "Y_test = coefficient_B_array @ X_test\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "#### Never forget the fact that  both `X_train` and `transpose(X_train)` are probably non-square matrices and their inverse is just not defined. However `transpose(X_train) @ X_train` is certainly a square matrix\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mean = np.mean(Y_train)\n",
    "\n",
    "Y_pred = X_train @ coefficient_B_array\n",
    "rss = (Y_pred - Y_train) @ (Y_pred - Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Implementing using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          2.00004783 -6.99995016 -6.00084238  9.00077378  0.99993754]\n",
      "16.562387823668814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2657.51244811])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg = reg.fit(X_train, Y_train)\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)\n",
    "\n",
    "Y_predicted = reg.predict([X_test])\n",
    "Y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
