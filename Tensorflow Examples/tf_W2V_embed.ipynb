{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_W2V_embed.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQSE6j1cPy4K",
        "colab_type": "code",
        "outputId": "59d172a6-bc62-426d-efc3-ca51324aae74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIP48DvihIg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import collections\n",
        "import math\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk4yXctbeNDU",
        "colab_type": "code",
        "outputId": "cb0efcf5-54c5-4f64-cb47-f3ed009bd5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w_WG-4DUIR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_words():\n",
        "  with zipfile.ZipFile(file='/content/gdrive/My Drive/SentimentAnalysisTensorFlow/SampleText.zip') as myZip:\n",
        "    firstFile = myZip.namelist()[0]\n",
        "    filestring = tf.compat.as_str((myZip.read(firstFile)))\n",
        "    words = filestring.split()\n",
        "\n",
        "  return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I7PoIf5guJ2",
        "colab_type": "code",
        "outputId": "5118e41f-8df7-4f7c-93a4-dc9f0fe0c556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocabulary = read_words()\n",
        "len(vocabulary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17005207"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve5XLF-phSmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building the dataset in useful format for word2vec embeddings\n",
        "# generating embeddings for only the top 'n' most frequently used words\n",
        "\n",
        "def buildDataset(words, n_words):\n",
        "  # A 2D array which holds the word and its frequency\n",
        "  word_counts = [['UNKNOWN', -1]]\n",
        "\n",
        "  counter = collections.Counter(words)\n",
        "  word_counts.extend(counter.most_common(n_words-1))\n",
        "  dictionary = dict()\n",
        "\n",
        "  for word, _ in word_counts:\n",
        "    # Assign unique indices to words\n",
        "    # The more common the word, the lower its index value\n",
        "    dictionary[word] = len(dictionary)\n",
        "\n",
        "  word_indices = list()\n",
        "\n",
        "  unknown_count = 0\n",
        "  for word in words:\n",
        "    if word in dictionary:\n",
        "      index = dictionary[word]\n",
        "    else:\n",
        "      index = 0\n",
        "      unknown_count += 1\n",
        "    word_indices.append(index)\n",
        "\n",
        "  word_counts[0][1] = unknown_count\n",
        "  reversed_dict = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "  return word_counts, word_indices, dictionary, reversed_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY_vXYYe_hPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildDataset2(words, n_words):\n",
        "  counter = collections.Counter(words)\n",
        "  word_frequency = dict(counter.most_common(n_words-1))\n",
        "  top_words_indices = dict()\n",
        "  word_indices = list()\n",
        "\n",
        "  unknow_count = 0\n",
        "  top_words_indices['UNKNOWN'] = 0\n",
        "\n",
        "  for word in word_frequency:\n",
        "    top_words_indices[word] = len(top_words_indices)\n",
        "\n",
        "  for word in words:\n",
        "    if word in top_words_indices:\n",
        "      index = top_words_indices[word]\n",
        "    else:\n",
        "      index = 0\n",
        "      unknow_count += 1\n",
        "    word_indices.append(index)\n",
        "\n",
        "  word_frequency['UNKNOWN'] = unknow_count\n",
        "  reversed_dict = dict(zip(top_words_indices.values(), top_words_indices.keys()))\n",
        "\n",
        "  return word_frequency, word_indices, top_words_indices, reversed_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn99g3-utpI9",
        "colab_type": "code",
        "outputId": "2647f61a-4551-4ed2-ddb2-07164206abb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "VOCAB_SIZE = 5000\n",
        "word_counts, word_indices, dictionary, reversed_dict = buildDataset(\n",
        "    words=vocabulary,\n",
        "    n_words=VOCAB_SIZE\n",
        ")\n",
        "word_indices[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrzBg0VpNcTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_index = 0\n",
        "# return a new batch of data for every iteration\n",
        "def generate_batch(word_indices, batch_size, num_skips, skip_window_size):\n",
        "  global global_index\n",
        "  assert batch_size % num_skips == 0\n",
        "  assert num_skips <= 2 * skip_window_size\n",
        "\n",
        "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "\n",
        "  # size of context window\n",
        "  # +1, so that it includes the input word\n",
        "  span = 2 * skip_window_size + 1\n",
        "\n",
        "  # text within the context window\n",
        "  buffer = collections.deque(maxlen=span)\n",
        "\n",
        "  for _ in range(span):\n",
        "    buffer.append(word_indices[global_index])\n",
        "    global_index = (global_index + 1) % len(word_indices)\n",
        "\n",
        "  # print(buffer)\n",
        "\n",
        "  for i in range(batch_size // num_skips):\n",
        "    target = skip_window_size\n",
        "    targets_to_avoid = [skip_window_size]\n",
        "\n",
        "    for j in range(num_skips):\n",
        "      while target in targets_to_avoid:\n",
        "        # choose a random index from span\n",
        "        # and add it to targets_to_avoid\n",
        "        # that is, if it already isn't there\n",
        "        target = random.randint(0, span-1)\n",
        "\n",
        "      targets_to_avoid.append(target)\n",
        "      batch[i * num_skips + j] = buffer[skip_window_size]\n",
        "      labels[i * num_skips + j, 0] = buffer[target]\n",
        "    \n",
        "    buffer.append(word_indices[global_index])\n",
        "    global_index = (global_index + 1) % len(word_indices)\n",
        "  \n",
        "  global_index = (global_index + len(word_indices) - span) % len(word_indices)\n",
        "  return batch, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWcYgB24t8UH",
        "colab_type": "code",
        "outputId": "d3b2bb57-4de9-45d1-de67-6bcd57d40e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "batch, labels = generate_batch(\n",
        "    word_indices=word_indices,\n",
        "    batch_size=10,\n",
        "    num_skips=2,\n",
        "    skip_window_size=5\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deque([0, 3081, 12, 6, 195, 2, 3134, 46, 59, 156, 128], maxlen=11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6MhiEqgH5RH",
        "colab_type": "code",
        "outputId": "b9361150-cb92-4e6d-a443-db64f6bc5226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "print(batch)\n",
        "print(labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   2    2 3134 3134   46   46   59   59  156  156]\n",
            "[[ 12]\n",
            " [195]\n",
            " [  6]\n",
            " [128]\n",
            " [128]\n",
            " [  6]\n",
            " [742]\n",
            " [156]\n",
            " [134]\n",
            " [477]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2EywVrYI1zp",
        "colab_type": "code",
        "outputId": "d557057a-79bd-42d6-f58e-fae7c72b2de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# input word is used to print two target words from its context window\n",
        "for i in range(9):\n",
        "  print(reversed_dict[batch[i]], \": \", reversed_dict[labels[i][0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "of :  as\n",
            "of :  term\n",
            "abuse :  a\n",
            "abuse :  early\n",
            "first :  early\n",
            "first :  a\n",
            "used :  working\n",
            "used :  against\n",
            "against :  including\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1c_bRqaI4f3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# within 100 words, pick 16 at random\n",
        "valid_size = 16\n",
        "valid_window = 100\n",
        "\n",
        "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ErK3-ab_vvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# no. of input words\n",
        "batch_size = 128\n",
        "# hidden layer will have 50 neurons\n",
        "embedding_size = 50\n",
        "skip_window_size = 2\n",
        "num_skips = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNCfb6od_-EP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHtV8o1gBU1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = tf.placeholder(dtype=tf.int32, shape=[batch_size])\n",
        "train_labels = tf.placeholder(dtype=tf.int32, shape=[batch_size, 1])\n",
        "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUiZdvklGETS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The embeddings are generated using training dataset\n",
        "# It contains an embedding of shape 1X50 for every word\n",
        "embeddings = tf.Variable(\n",
        "    initial_value=tf.random_uniform(\n",
        "        shape=[VOCAB_SIZE, embedding_size],\n",
        "        minval=-1.0,\n",
        "        maxval=1.0\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu0m1IzBHHWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For every iteration in training, we can only generate\n",
        "# or train embeddings for the words in that particular batch\n",
        "\n",
        "# The word inputs in every training batch will look up the\n",
        "# embeddings for those words in the embedding matrix\n",
        "\n",
        "# 'train_inputs' placeholder contains the unique word indices\n",
        "# in this batch and they are looked up in the embeddings matrix\n",
        "embed = tf.nn.embedding_lookup(params=embeddings, ids=train_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32-_FxGYUByn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up a hidden layer by using ops y = Wx + b\n",
        "weights = tf.Variable(tf.truncated_normal(\n",
        "    shape=[VOCAB_SIZE, embedding_size],\n",
        "    stddev=1.0 / math.sqrt(embedding_size)\n",
        "    ))\n",
        "\n",
        "biases = tf.Variable(initial_value=tf.zeros(shape=[VOCAB_SIZE]))\n",
        "\n",
        "# A neural network with no activation function\n",
        "# that is, a linear layer\n",
        "hidden_output = tf.matmul(embed, tf.transpose(weights)) + biases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdfPXgE6VGX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_one_hot = tf.one_hot(train_labels, VOCAB_SIZE)\n",
        "loss = tf.reduce_mean(\n",
        "    input_tensor=tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        logits=hidden_output,\n",
        "        labels=train_one_hot\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzqM3NPOlLy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C5iNOuErvDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l2_norm = tf.linalg.norm(ord='euclidean', tensor=embeddings)\n",
        "normalized_embeddings = embeddings /l2_norm\n",
        "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q4ajqtJuzeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# No of corrupted sample pairs\n",
        "# to be fed into NCE\n",
        "num_samples = 64\n",
        "num_steps = 20001\n",
        "\n",
        "nce_weights = tf.Variable(tf.truncated_normal(\n",
        "    shape=[VOCAB_SIZE, embedding_size],\n",
        "    stddev=1.0 / math.sqrt(embedding_size)\n",
        "    ))\n",
        "\n",
        "nce_biases = tf.Variable(initial_value=tf.zeros(shape=[VOCAB_SIZE]))\n",
        "\n",
        "nce_loss = tf.reduce_mean(\n",
        "    input_tensor=tf.nn.nce_loss(\n",
        "        weights=nce_weights,\n",
        "        biases=nce_biases,\n",
        "        labels=train_labels,\n",
        "        inputs=embed,\n",
        "        num_sampled=num_samples,\n",
        "        num_classes=VOCAB_SIZE\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjTMCvFQ1CdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nce_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(nce_loss)\n",
        "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRDLj8_74PuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GdzFSSZ3MFv",
        "colab_type": "code",
        "outputId": "eea621ad-db21-4ad3-a499-6067ba567d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "\n",
        "  avg_loss = 0\n",
        "  for step in range(num_steps):\n",
        "    batch_inputs, batch_labels = generate_batch(\n",
        "        word_indices,\n",
        "        batch_size,\n",
        "        num_skips,\n",
        "        skip_window_size\n",
        "    )\n",
        "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
        "    _, loss_val = sess.run([nce_optimizer, nce_loss], feed_dict=feed_dict)\n",
        "    avg_loss += loss_val\n",
        "\n",
        "    if step % 2000 == 0 and step != 0:\n",
        "      avg_loss /= 2000\n",
        "      print(\"Avg loss at step\", step, ': ', avg_loss)\n",
        "      avg_loss=0\n",
        "    \n",
        "    if step % 10000 == 0:\n",
        "      sim = similarity.eval()\n",
        "\n",
        "      for i in range(valid_size):\n",
        "        valid_word = reversed_dict[valid_examples[i]]\n",
        "        top_k = 8\n",
        "\n",
        "        nearest = (-sim[i, :]).argsort()[1: top_k + 1]\n",
        "        log_str = 'Nearest to %s' % valid_word\n",
        "\n",
        "        for k in range(top_k):\n",
        "          close_word = reversed_dict[nearest[k]]\n",
        "          log_str = '%s %s, ' % (log_str, close_word)\n",
        "        print(log_str)\n",
        "      print(\"\\n\")\n",
        "  final_embeddings = normalized_embeddings.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nearest to not stones,  mostly,  exile,  levels,  du,  faster,  masters,  printed, \n",
            "Nearest to were infant,  birds,  pakistan,  unable,  hotel,  wrong,  brazil,  warner, \n",
            "Nearest to be mine,  evening,  fell,  crusade,  put,  milton,  hot,  jurisdiction, \n",
            "Nearest to will george,  concept,  leaving,  bread,  foundations,  merely,  production,  pierre, \n",
            "Nearest to during salt,  los,  ira,  digital,  fundamental,  ohio,  painting,  arrangement, \n",
            "Nearest to i dialects,  assumed,  physician,  substances,  unified,  needed,  playing,  branches, \n",
            "Nearest to which laser,  attributes,  regular,  editions,  approach,  hindu,  split,  occupied, \n",
            "Nearest to d and,  communism,  industrial,  worked,  officer,  manufactured,  unit,  dynamic, \n",
            "Nearest to three operators,  norwegian,  wisconsin,  leo,  script,  indo,  simple,  destroyed, \n",
            "Nearest to between father,  bringing,  heavily,  successor,  songwriter,  requires,  comments,  motion, \n",
            "Nearest to known chris,  rear,  role,  consequences,  cult,  concerning,  mark,  vi, \n",
            "Nearest to first favour,  scheduled,  easy,  economy,  dave,  published,  ties,  egypt, \n",
            "Nearest to had doesn,  diego,  floppy,  bowl,  chosen,  liverpool,  consequently,  generation, \n",
            "Nearest to seven confused,  household,  jimmy,  emperors,  catalan,  row,  additionally,  death, \n",
            "Nearest to all prussian,  occurring,  buried,  artists,  two,  program,  emergency,  produces, \n",
            "Nearest to would been,  remainder,  accomplished,  recovered,  imports,  bosnia,  ferdinand,  testing, \n",
            "\n",
            "\n",
            "Avg loss at step 2000 :  201.1314751358032\n",
            "Avg loss at step 4000 :  200.6745739517212\n",
            "Avg loss at step 6000 :  200.497066116333\n",
            "Avg loss at step 8000 :  200.1372109146118\n",
            "Avg loss at step 10000 :  200.25426271057128\n",
            "Nearest to not seven,  zero,  five,  nine,  three,  four,  eight,  also, \n",
            "Nearest to were nine,  seven,  four,  eight,  three,  five,  six,  zero, \n",
            "Nearest to be nine,  six,  four,  zero,  eight,  seven,  three,  also, \n",
            "Nearest to will seven,  eight,  nine,  three,  six,  who,  five,  four, \n",
            "Nearest to during zero,  nine,  six,  four,  eight,  seven,  two,  who, \n",
            "Nearest to i eight,  nine,  zero,  five,  three,  four,  two,  seven, \n",
            "Nearest to which six,  zero,  eight,  three,  seven,  nine,  four,  english, \n",
            "Nearest to d seven,  four,  zero,  eight,  six,  nine,  five,  who, \n",
            "Nearest to three five,  nine,  eight,  seven,  six,  four,  zero,  two, \n",
            "Nearest to between five,  nine,  seven,  zero,  three,  four,  six,  eight, \n",
            "Nearest to known known,  six,  five,  seven,  nine,  four,  eight,  three, \n",
            "Nearest to first seven,  three,  eight,  six,  five,  four,  nine,  zero, \n",
            "Nearest to had six,  four,  seven,  five,  three,  zero,  nine,  would, \n",
            "Nearest to seven four,  nine,  zero,  six,  eight,  five,  three,  two, \n",
            "Nearest to all seven,  two,  eight,  zero,  six,  nine,  five,  four, \n",
            "Nearest to would six,  zero,  seven,  eight,  been,  had,  nine,  are, \n",
            "\n",
            "\n",
            "Avg loss at step 12000 :  199.92184023284912\n",
            "Avg loss at step 14000 :  200.70292014312744\n",
            "Avg loss at step 16000 :  200.45500679779053\n",
            "Avg loss at step 18000 :  200.31676735687256\n",
            "Avg loss at step 20000 :  200.2740101928711\n",
            "Nearest to not university,  apple,  british,  u,  such,  war,  n,  year, \n",
            "Nearest to were university,  based,  case,  nine,  seven,  because,  where,  see, \n",
            "Nearest to be six,  others,  found,  british,  university,  english,  also,  year, \n",
            "Nearest to will apple,  seven,  austin,  well,  african,  land,  british,  would, \n",
            "Nearest to during university,  austin,  time,  apple,  made,  six,  who,  language, \n",
            "Nearest to i eight,  ii,  english,  used,  called,  being,  god,  nine, \n",
            "Nearest to which english,  six,  eight,  series,  become,  b,  form,  known, \n",
            "Nearest to d seven,  university,  english,  four,  who,  often,  apple,  eight, \n",
            "Nearest to three five,  eight,  nine,  six,  seven,  four,  amiga,  then, \n",
            "Nearest to between father,  used,  th,  five,  apple,  so,  original,  seven, \n",
            "Nearest to known english,  b,  example,  six,  called,  ii,  american,  out, \n",
            "Nearest to first amiga,  up,  seven,  published,  apple,  angels,  six,  day, \n",
            "Nearest to had english,  university,  would,  american,  six,  seven,  more,  apollo, \n",
            "Nearest to seven six,  eight,  four,  d,  th,  b,  nine,  university, \n",
            "Nearest to all seven,  program,  angels,  number,  d,  r,  language,  university, \n",
            "Nearest to would been,  six,  american,  english,  may,  university,  atari,  written, \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgtmasLP5ho7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_embeddings(embeddings, word_counts):\n",
        "  np.save('embeddings.npy', embeddings)\n",
        "  words = [x[0] for x in word_counts]\n",
        "  np.save('words.npy', words)\n",
        "\n",
        "save_embeddings(final_embeddings, word_counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rc3WanBK-y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}